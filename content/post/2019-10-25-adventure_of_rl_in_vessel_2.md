+++
title = "강화학습의 혈관 속 탐험 (2) - 강화학습과 제어 이론의 비교"
summary = "Introduction to guide-wire control for PCI by RL (2) - RL vs Control Theory"
date = 2019-10-25T13:00:00+09:00
draft = false
authors=["chaehyeuk-lee", "kyunghwan-kim"]
# Tags and categories
# For example, use `tags = []` for no tags, or the form `tags = ["A Tag", "Another Tag"]` for one or more tags.
tags = []
categories = []

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
[image]
  # Caption (optional)
  caption = ""

  # Focal point (optional)
  # Options: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight
  focal_point = ""
  
+++

이번 포스트에서는 왜 저희가 PCI 로봇 제어 방법으로 강화학습을 선택하게 되었는지 얘기해보고자 합니다.
이미 수많은 학자와 엔지니어들이 로봇을 위한 효율적이고 뛰어난 제어 이론들을 개발해왔습니다.  
이러한 제어 이론들이 어떠한 점에서 강화학습과 다르고, 어떠한 장단점으로 인해 저희가 강화학습을 선택하게 되었는지 이야기해보겠습니다.  
제일 먼저, 제어 이론 중 가장 보편적이고 널리 쓰이는 PID 제어와의 비교로 시작하겠습니다.

### PID Control
---
PID 제어는 Proportional-Integral-Derivative Control의 약자입니다. 현재 산업 현장의 80~90% 이상의 제어기는 이 PID제어 이론을 이용하여 동작을 수행합니다. 그만큼 이 PID 제어에는 장점이 많습니다. 먼저 알고리즘이 코드 10줄 안에 끝날 정도로 가볍고, 사용하기 쉬우며, 제어 성능이 뛰어납니다. 하지만 이 PID 제어는 단순한 만큼 한가지 문제에만 집중합니다. 바로 **Tracking** 문제입니다.

아래 식은 PID 제어를 표현하는 가장 기본적인 식입니다. 아래 식을 살펴보면, 최종 출력 ut는 목표 상태와 현재 상태의 차이, et에 의해서 결정되는 것을 알 수 있습니다. 즉, PID 제어는 목표 상태와 현재 상태를 가장 최단거리로 잇는 경로 위에서만 동작합니다.
그림과 함께 좀더 자세히 이야기해보겠습니다. 아래 그림에 목표 위치와 현재 위치가 있습니다. PID제어를 이용하면, 그림 ?과 같은 경로로 현재 상태가 이동하게 됩니다. 제어기 설계에 따라서 속도가 느려지거나 빨라질 수도, 목표 지점을 넘어갈 수도 있습니다. 하지만 직선 l에서 벗어나지는 않습니다. 예측 불가능한 외란으로 인해 x2와 같이 경로에서 벗어날 수 있습니다. 하지만 PID제어기는 그 경로에서 벗어난 현재 상태로부터 다시 직선 l2을 만들고, 그 직선을 따라 움직이려고 합니다. 즉 목표 지점이 바뀌지 않는 한, 현재 상태가 움직이는 경로는 바뀌지 않습니다.

따라서 PID제어를 이용하여 복잡한 시스템을 제어하기 위해서는, 상태에 따라 목표 지점을 변경해주는 알고리즘, 즉 **경로**를 생성해주는 알고리즘이 추가로 필요합니다. 아래와 같은 단순한 로봇팔을 생각해보겠습니다. 로봇팔이 목표 지점을 향해 움직일 때, 로봇 핸드가 직선으로 움직이려면 1번 축과 2번 축은 서로 다른 방향으로 움직여야합니다. 이 각 축의 움직이는 방향과 속도, 즉 **최적 경로**는 기구학이라는 추가적인 알고리즘을 이용하여 생성해야 합니다. 


### Problem in PID Control
---
이처럼 PID 제어를 적용하기 위해서는, **경로**를 찾는 문제를 먼저 풀어야합니다. 문제는 PCI 문제에서 가이드와이어의 최적 경로를 찾는 일이 매우 어렵다는 것입니다.

가이드와이어를 이용한 PCI 로봇 제어의 가장 큰 문제 중 하나는, 가이드와이어가 어떠한 경로로 움직여야 목표 지점까지 도달할 수 있는지 직관적으로 알기 어렵다는 것입니다. 쉽게 생각하면 아래 그림 ?와 같이 혈관의 중심선을 따라 움직이면 될 것 같습니다. 하지만 실제로는 가이드와이어와 카데터 사이의 마찰, 혈관 내벽과의 마찰 등으로 인해 중심선만 따라가서는 목표까지 도달할 수 없습니다. 아래 그림 ?는 저희가 수행한 실험 중 하나로, 한개의 분지 혈관을 통과하기 위해 가이드와이어가 선택한 경로를 보여줍니다. [참고문헌 ?] 그림을 보시면 가이드와이어의 경로는 중심선이 아니라, 대부분 혈관 벽에서 벽으로 이동하여 **마찰력을 이용해** 움직이는 것을 알 수 있습니다. 이러한 경로는 수학적인 방법이나 직관적으로 얻어내기 매우 어렵습니다. 따라서 최적 경로를 쉽게 만들어낼 수 없고, 최적 경로를 얻어낼 수 없다면 PID 제어를 적용하기 어렵습니다.

### Reinforcement Learning 
강화학습은 이러한 문제를 푸는데 최적화되어 있습니다. 사실 강화학습과 PID 제어는 동일선 상에서 비교하는 것이 불공평할 수도 있습니다. 두 알고리즘이 각각 목표하는 바가 다르기 때문입니다. PID 제어는 위에서 말씀드렸듯이 **Tracking**에 최적화되어 있고, 강화학습은 **최적 경로**를 찾는 것에 최적화되어 있습니다. 
강화학습이 목표로 하는 것은 에이전트가 각 상태(state)에서 보상(reward)을 최대로 하는 행동(action)을 하는 것입니다. 이러한 목표를 달성하기 위해 각 상태에서 어떤 행동을 선택하는 것이 좋은지 확률로 표현하고 이를 정책(policy)라 부릅니다. 에이전트는 환경과 상호작용하며 얻는 보상을 최대로 하는 방향으로 정책을 업데이트 해나갑니다. 즉 강화학습 에이전트는 문제에 대한 최적의 정책을 찾게 되고 이는 곧 최적 경로를 찾는 것과 같습니다. 우리가 미로찾기에서 갈림길에 도달했을 때 항상 목표지점으로 가는 방향으로 갈 수 있는 길을 고른다면 우리가 지나온 갈림길들은 목표지점으로 가는 최적의 길이 되기 때문입니다.

### PID Control vs Reinforcement Learning
지금까지 왜 PID 제어로는 PCI 로봇을 제어하기 어려운지 알아보았습니다. 이 두 알고리즘의 차이는 결국 **최적 경로**를 만들어낼 수 있는가 없는가의 차이입니다. 강화학습은 **병변부위까지 가이드와이어를 이동**시키는 최적의 경로를 찾아주는 것이 가능하지만, PID 제어로는 그 경로를 찾을 수 없습니다. \\
하지만 독자분들께서는 여기서 의문을 제기하실 수 있습니다. 왜냐하면 기존 제어이론에도 최적 경로를 찾는 알고리즘이 있기 때문입니다. 이는 **최적 제어 (Optimal Control)**이라는 이론입니다. 아래부터는 이 최적제어 이론이 강화학습과 어떻게 다른지, 왜 저희는 최적경로를 위한 두 알고리즘 중에서도 강화학습을 선택하게 되었는지 이야기해보겠습니다.


### Optimal Control
이번 글에서는 지난 PID제어와 강화학습의 차이에 대한 글에 이어, 최적제어와 강화학습을 비교해보겠습니다.
이전 글에서 이야기한 것처럼, 강화학습과 Optimal Control은 모두 **최적 경로**를 찾는데 최적화된 알고리즘입니다.
목적이 같은 만큼 두 알고리즘은 많은 면에서 유사한 모습을 보입니다.
실제로 강화학습의 아버지라고 불리는 Richard S. Sutton 교수는 최적제어의 한 부분으로서 강화학습을 활용하는 것을 제안하기도 했습니다. [참고문헌]

먼저 강화학습과 최적 제어의 공통점부터 얘기해보도록 하겠습니다.
두 분야 모두 Dynamic Programming이라는 알고리즘을 시작점으로 삼습니다.
Dynamic Programming이란 Richard Ernest Bellman이란 학자가 제안한 Bellman’s principle of optimality 를 기반으로 최적 경로를 찾는 방법을 말합니다.
강화학습과 최적 제어가 각각 이 Dynamic Programming을 활용하는 방법은 용어와 계산하는 과정만 조금 차이가 있을 뿐, 거의 동일한 개념과 구조를 갖고 있습니다.
(본 글에서 Pontryagin's Minimum Principle (PMP)은 다루지 않을 예정입니다. 필자가 이에 대해 깊은 이해가 없기도 하고, Dynamic Programming에서 하고자 하는 이야기가 PMP에도 그대로 적용되기 때문입니다.)

#### 강화학습 Dynamic Programming 

#### 최적제어 Dynamic Programming 

### Model-free를 향해
위의 두 알고리즘의 구조를 보시면, 모델의 형태와 사용하는 목적함수만 다를 뿐, 그 구조는 완전히 동일하다고 해도 무방합니다. 
Dynamic Programming에서 두 방법은 모두 **모델**이 있다는 것을 가정하고 문제를 풀어나갑니다.
최적제어에서는 상태 공간 방정식을 이용해 모델을 표현하고, 강화학습에서는 상태, 행동, 상태 전이 확률로 모델을 표현합니다.
이 모델을 이용하여 목적 함수의 최종 결과를 계산하고, 그 결과에 따라 로봇에 적용되는 입력을 결정합니다.
만약 저희가 이 **모델**을 구할 수 없는 상황이 되면 어떻게 될까요?
단순히 파라미터의 값이 조금 틀린 정도가 아니라, 모델에 영향을 미치는 요소가 너무 많아 모델에 다 담을 수 없다면? 
실제 환경의 제약으로 인해 그 요소들을 모두 측정할 수 없는 상황에서는 어떠한 방법을 써야 할까요?
강화학습은 이 문제를 trial & error, 즉 경험을 이용한 방법으로 풀어냅니다.
Dynamic Programming 이후의 강화학습 이론, Monte-Carlo, Q-learning 등이 그 예입니다.
실제 경험을 이용해 각 상태에서 목적 함수의 측정값을 수집한 후에, 목적 함수의 예측값과 실제 측정값이 같아지도록 목적함수를 점점 변화시킵니다. 
목적 함수를 계산하는데 모델을 사용하지 않습니다.
즉, **Model-free**조건에서 문제를 푸는 방법입니다.

PCI 로봇 제어는 이러한 Model-free 방법이 필요합니다.
PCI 로봇의 구조를 보면, 아래 그림과 같이 PCI 입구까지 카데터가 있고, 가이드와이어를 카데터 내부 관을 통해 PCI 내부 혈관까지 이동시키는 구조를 갖고 있습니다. 
로봇은 카데터의 시작 단에서 가이드와이어를 조작합니다.
이러한 경우 모델을 만드려면 가이드와이어에 작용하는 힘 요소들을 모두 알아야 하는데, 가이드와이어의 어느 부분이 휘어져있는지, 어느 부분이 혈관과 카데터와 닿아있는지, 가이드와이어와 혈관 사이 마찰력은 얼마나 강한지 등을 모두 알아야합니다.
이를 모두 측정할 수 있으면 좋겠지만, 아쉽게도 현재 판매 중인 가이드와이어 중 그 모든 것을 측정할 수 있는 제품은 없습니다.
심지어 이 요소들은 시간에 따라 불규칙하게 변하는, time-variant 특성을 갖고 있고, 그 변화 폭이 커 모델로 표현하기가 더더욱 어렵습니다. 
결국 이러한 요소들을 경험을 통해서 알아내야 한다는 결론에 저희는 도달했습니다.

#### Conclusion
이번 포스트에서는 최적경로를 찾는 것을 목표로 하는 두가지 방법, 최적제어와 강화학습 중 저희가 왜 강화학습을 선택했는지 이야기해보았습니다.
Model-free 접근이 가능하다는 것이 저희가 PCI 로봇에 강화학습을 적용한 가장 큰 이유입니다.
다음 글에서는 드디어 저희가 선택한 강화학습을 실제로 어떻게 PCI 로봇에 적용했는지 이야기해보겠습니다.


